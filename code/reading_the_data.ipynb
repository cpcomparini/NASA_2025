{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"vgZuy4bg26N1"},"id":"vgZuy4bg26N1","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":19,"id":"810c14bb-18b0-460b-b166-b46b6f07169a","metadata":{"id":"810c14bb-18b0-460b-b166-b46b6f07169a","executionInfo":{"status":"ok","timestamp":1759710811114,"user_tz":180,"elapsed":31,"user":{"displayName":"Caue Comparini","userId":"10395338309022687680"}}},"outputs":[],"source":["import warnings, pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n","pd.set_option('display.max_rows', 100)  # Mostra todas as linhas\n","pd.set_option('display.max_columns', 100)  # Mostra todas as colunas"]},{"cell_type":"code","execution_count":20,"id":"76d38ac0-36ef-4830-9bcb-279d65de8822","metadata":{"id":"76d38ac0-36ef-4830-9bcb-279d65de8822","executionInfo":{"status":"ok","timestamp":1759710812491,"user_tz":180,"elapsed":739,"user":{"displayName":"Caue Comparini","userId":"10395338309022687680"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"27853389-1360-4752-a872-6353723e3448"},"outputs":[{"output_type":"stream","name":"stdout","text":["(7703, 87)\n"]}],"source":["header = {\n","    \"rowid\": \"rowid\",\n","    \"toi\":            \"TESS Object of Interest\",\n","\"toipfx\":         \"TESS Object of Interest Prefix\",\n","\"tid\":            \"TESS Input Catalog ID\",\n","\"ctoi_alias\":     \"TESS Input Catalog of Interest Alias\",\n","\"pl_pnum\":        \"Pipeline Signal ID\",\n","\"tfopwg_disp\":    \"TFOPWG Dispostion (CP | FP | KP | PC)\",\n","\"rastr\":          \"RA [sexagesimal]\",\n","\"ra\":             \"RA [deg]\",\n","\"raerr1\":         \"RA Upper Unc\",\n","\"raerr2\":         \"RA Lower Unc\",\n","\"decstr\":         \"Dec [sexagesimal]\",\n","\"dec\":            \"Dec [deg]\",\n","\"decerr1\":        \"Dec Upper Unc\",\n","\"decerr2\":        \"Dec Lower Unc\",\n","\"st_pmra\":        \"PMRA [mas/yr]\",\n","\"st_pmraerr1\":    \"PMRA Upper Unc [mas/yr]\",\n","\"st_pmraerr2\":    \"PMRA Lower Unc [mas/yr]\",\n","\"st_pmralim\":     \"PMRA Limit Flag\",\n","\"st_pmrasymerr\":  \"PMRA Symmetric Error Flag\",\n","\"st_pmdec\":       \"PMDec [mas/yr]\",\n","\"st_pmdecerr1\":   \"PMDec Upper Unc [mas/yr]\",\n","\"st_pmdecerr2\":   \"PMDec Lower Unc [mas/yr]\",\n","\"st_pmdeclim\":    \"PMDec Limit Flag\",\n","\"st_pmdecsymerr\": \"PMDec Symmetric Error Flag\",\n","\"pl_tranmid\":     \"Planet Transit Midpoint Value [BJD]\",\n","\"pl_tranmiderr1\": \"Planet Transit Midpoint Upper Unc [BJD]\",\n","\"pl_tranmiderr2\": \"Planit Transit Midpoint Lower Unc [BJD]\",\n","\"pl_tranmidlim\":  \"Planet Transit Midpoint Limit Flag\",\n","\"pl_tranmidsymerr\": \"Planet Transit Midpoint Symmetric Error Flag\",\n","\"pl_orbper\":      \"Planet Orbital Period Value [days]\",\n","\"pl_orbpererr1\":  \"Planet Orbital Period Upper Unc [days]\",\n","\"pl_orbpererr2\":  \"Planet Orbital Period Lower Unc [days]\",\n","\"pl_orbperlim\":   \"Planet Orbital Period Limit Flag\",\n","\"pl_orbpersymerr\": \"Planet Orbital Period Symmetric Error Flag\",\n","\"pl_trandurh\":    \"Planet Transit Duration Value [hours]\",\n","\"pl_trandurherr1\": \"Planet Transit Duration Upper Unc [hours]\",\n","\"pl_trandurherr2\": \"Planet Transit Duration Lower Unc [hours]\",\n","\"pl_trandurhlim\": \"Planet Transit Duration Limit Flag\",\n","\"pl_trandurhsymerr\": \"Planet Transit Duration Symmetric Error Flag\",\n","\"pl_trandep\":     \"Planet Transit Depth Value [ppm]\",\n","\"pl_trandeperr1\": \"Planet Transit Depth Upper Unc [ppm]\",\n","\"pl_trandeperr2\": \"Planet Transit Depth Lower Unc [ppm]\",\n","\"pl_trandeplim\":  \"Planet Transit Depth Limit Flag\",\n","\"pl_trandepsymerr\": \"Planet Transit Depth Symmetric Error Flag\",\n","\"pl_rade\":        \"Planet Radius Value [R_Earth]\",\n","\"pl_radeerr1\":    \"Planet Radius Upper Unc [R_Earth]\",\n","\"pl_radeerr2\":    \"Planet Radius Lower Unc [R_Earth]\",\n","\"pl_radelim\":     \"Planet Radius Limit Flag\",\n","\"pl_radesymerr\":  \"Planet Radius Symmetric Error Flag\",\n","\"pl_insol\":       \"Planet Insolation Value [Earth flux]\",\n","\"pl_insolerr1\":   \"Planet Insolation Upper Unc [Earth flux]\",\n","\"pl_insolerr2\":   \"Planet Insolation Lower Unc [Earth flux]\",\n","\"pl_insollim\":    \"Planet Insolation Limit Flag\",\n","\"pl_insolsymerr\": \"Planet Insolation Symmetric Error Flag\",\n","\"pl_eqt\":         \"Planet Equilibrium Temperature Value [K]\",\n","\"pl_eqterr1\":     \"Planet Equilibrium Temperature Upper Unc [K]\",\n","\"pl_eqterr2\":     \"Planet Equilibrium Temperature Lower Unc [K]\",\n","\"pl_eqtlim\":      \"Planet Equilibrium Temperature Limit Flag\",\n","\"pl_eqtsymerr\":   \"Planet Equilibrium Temperature Symmetric Error Flag\",\n","\"st_tmag\":        \"TESS Magnitude\",\n","\"st_tmagerr1\":    \"TESS Magnitude Upper Unc\",\n","\"st_tmagerr2\":    \"TESS Magnitude Lower Unc\",\n","\"st_tmaglim\":     \"TESS Magnitude Limit Flag\",\n","\"st_tmagsymerr\":  \"TESS Magnitude Symmetric Error Flag\",\n","\"st_dist\":        \"Stellar Distance [pc]\",\n","\"st_disterr1\":    \"Stellar Distance Upper Unc [pc]\",\n","\"st_disterr2\":    \"Stellar Distance Lower Unc [pc]\",\n","\"st_distlim\":     \"Stellar Distance Limit Flag\",\n","\"st_distsymerr\":  \"Stellar Distance Symmetric Error Flag\",\n","\"st_teff\":        \"Stellar Effective Temperature Value [K]\",\n","\"st_tefferr1\":    \"Stellar Effective Termperature Upper Unc [K]\",\n","\"st_tefferr2\":    \"Stellar Effective Temperature Lower Unc [K]\",\n","\"st_tefflim\":     \"Stellar Effective Temperature Limit Flag\",\n","\"st_teffsymerr\":  \"Stellar Effective Temperature Symmetric Error Flag\",\n","\"st_logg\":        \"Stellar log(g) Value [cm/s**2]\",\n","\"st_loggerr1\":    \"Stellar log(g) Upper Unc [cm/s**]\",\n","\"st_loggerr2\":    \"Stellar log(g) Lower Unc [cm/s**2]\",\n","\"st_logglim\":     \"Stellar log(g) Limit Flag\",\n","\"st_loggsymerr\":  \"Stellar log(g) Symmetric Error Flag\",\n","\"st_rad\":         \"Stellar Radius Value [R_Sun]\",\n","\"st_raderr1\":     \"Stellar Radius Upper Unc [R_Sun]\",\n","\"st_raderr2\":     \"Stellar Radius Lower Unc [R_Sun]\",\n","\"st_radlim\":      \"Stellar Radius Limit Flag\",\n","\"st_radsymerr\":   \"Stellar Radius Symmetric Error Flag\",\n","\"toi_created\":    \"TOI Created Date\",\n","\"rowupdate\":      \"Date Modified\"\n","}\n","data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NSAC25/data/raw/tess_objects_of_interest.csv\",\n","                  comment = \"#\"\n","                  )\n","print(data.shape)\n","\n","# TESS Follow-up Observing Program Working Group (TFOPWG) Dispostion:\n","# APC = ambiguous planetary candidate\n","# CP = confirmed planet\n","# FA = false alarm\n","# FP = false positive\n","# KP = known planet\n","# PC = planetary candidate"]},{"cell_type":"code","execution_count":null,"id":"2fe6a15d-03f2-4411-824a-105ca8a2b773","metadata":{"id":"2fe6a15d-03f2-4411-824a-105ca8a2b773"},"outputs":[],"source":["# Chosen manually, one by one, out of the 87 available.\n","tess_identification_columns = [\"ctoi_alias\", \"pl_pnum\", \"tfopwg_disp\", 'ra', 'dec', 'st_pmra', 'st_pmdec']\n","planetary_properties = ['pl_tranmid', 'pl_orbper', 'pl_trandurh', 'pl_trandep', 'pl_rade', 'pl_insol', 'pl_eqt']\n","stellar_properties = ['st_tmag', 'st_dist', 'st_teff', 'st_logg', 'st_rad']\n","\n","columns_kept = [*tess_identification_columns, *planetary_properties, *stellar_properties]\n","data = data[columns_kept]\n","print(data.shape)"]},{"cell_type":"code","execution_count":null,"id":"b676e39b-0df5-45bc-b5b4-523789df4a98","metadata":{"id":"b676e39b-0df5-45bc-b5b4-523789df4a98"},"outputs":[],"source":["# Label Encoding\n","data = data[data['tfopwg_disp'].values != 'PC']\n","data = data[data['tfopwg_disp'].values != 'APC']\n","\n","with warnings.catch_warnings():\n","    warnings.simplefilter('ignore')\n","\n","    data['tfopwg_disp'][data['tfopwg_disp'].values == 'KP'] = 1\n","    data['tfopwg_disp'][data['tfopwg_disp'].values == 'CP'] = 1\n","    data['tfopwg_disp'][data['tfopwg_disp'].values == 'FA'] = 0\n","    data['tfopwg_disp'][data['tfopwg_disp'].values == 'FP'] = 0\n","\n","data['tfopwg_disp'] = data['tfopwg_disp'].astype('int64')\n","print(data.shape)"]},{"cell_type":"code","execution_count":null,"id":"3e31dbba-b089-4a4c-987f-faa5705be8e4","metadata":{"id":"3e31dbba-b089-4a4c-987f-faa5705be8e4"},"outputs":[],"source":["print(f\"Formato do DataFrame: {data.shape}\")  # (m_linhas, n_colunas)\n","print(f\"Número de linhas: {len(data)}\")\n","print(f\"Número de colunas: {len(data.columns)}\")\n","print(f\"Memória usada: {data.memory_usage(deep=True).sum() / (1024 ** 2):.2f} MB\")\n","\n","percentages_of_missing_data = data.isnull().mean() * 100 #percentages of NaN values in each column\n","bad_columns = [header[key] for key in percentages_of_missing_data[percentages_of_missing_data.values > 30].index] # bad columns are those which have 30% or more NaNs\n","\n","print(\"\\nBAD COLUMNS (DROPPED):\")\n","for _ in bad_columns: print(_)\n","bad_columns = [key for key in percentages_of_missing_data[percentages_of_missing_data.values > 30].index]\n","data = data.drop(columns = bad_columns)"]},{"cell_type":"code","execution_count":null,"id":"9f133492-e42c-4d6c-a158-eb3ab5023f7a","metadata":{"id":"9f133492-e42c-4d6c-a158-eb3ab5023f7a"},"outputs":[],"source":["# This cell was written with the help of Mistral AI's LeChat chatbot\n","numeric_cols = data.select_dtypes(include=[np.number]).columns\n","\n","for col in numeric_cols:\n","    Q1 = data[col].quantile(0.25)\n","    Q3 = data[col].quantile(0.75)\n","    IQR = Q3 - Q1\n","    lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n","\n","    avg = data[col].mean()\n","    s_dev = data[col].std()\n","    lower_2, upper_2 = avg - 3 * s_dev, avg + 3 * s_dev\n","\n","    plt.figure(figsize=(8, 4))\n","    sns.histplot(data[col], kde=True)\n","\n","    plt.axvline(lower, color='red', linestyle='--', label='Limite Inferior IQR')\n","    plt.axvline(upper, color='red', linestyle='--', label='Limite Superior IQR')\n","    plt.axvline(lower_2, color='purple', linestyle='--', label=r'Limite Inferior $3\\sigma$')\n","    plt.axvline(upper_2, color='purple', linestyle='--', label=r'Limite Superior $3\\sigma$')\n","\n","    plt.title(f\"Distribuição de {col} com limites de outliers\")\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"7c9c260d-53f5-4eda-8486-6b31bb45e1d0","metadata":{"id":"7c9c260d-53f5-4eda-8486-6b31bb45e1d0"},"outputs":[],"source":["# This cell was written with the help of Mistral AI's LeChat chatbot\n","\n","# Removal of outliers\n","data_to_keep = np.array([True for i in range(len(data))])\n","for col in data.columns:\n","    avg = data[col].mean()\n","    s_dev = data[col].std()\n","    lower, upper = avg - 3 * s_dev, avg + 3 * s_dev\n","    temp = (data[col].values > lower) & (data[col].values < upper)\n","    data_to_keep = data_to_keep & temp\n","data = data[data_to_keep]"]},{"cell_type":"code","source":["# The four cells below were written with the help of Mistral AI's LeChat chatbot"],"metadata":{"id":"7Pt68yizog_E"},"id":"7Pt68yizog_E","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import missingno as msno\n","msno.matrix(data)  # Matriz de missing values (gráfico interativo)"],"metadata":{"id":"dy95Du0H91tw"},"id":"dy95Du0H91tw","execution_count":null,"outputs":[]},{"cell_type":"code","source":["msno.bar(data)     # Gráfico de barras"],"metadata":{"id":"DtAVhs3Y_GDP"},"id":"DtAVhs3Y_GDP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["msno.heatmap(data)  # Mostra correlação entre missing values"],"metadata":{"id":"8GBk6W-m-DW_"},"id":"8GBk6W-m-DW_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.isnull().sum()"],"metadata":{"id":"aZxvR5xPUJng"},"id":"aZxvR5xPUJng","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The four cells above were written with the help of Mistral AI's LeChat chatbot"],"metadata":{"id":"CFbHtJ91ojdI"},"id":"CFbHtJ91ojdI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Luckily for me, the outlier removal has also removed all remaining missing numbers\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from imblearn.over_sampling import SMOTE\n","\n","# Separate features from targets\n","X = data.drop(['ctoi_alias', 'pl_pnum', 'tfopwg_disp'], axis=1)\n","y = data['tfopwg_disp']\n","\n","# Rescale Features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Train test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_scaled, y,\n","    test_size = 0.2,\n","    stratify = y,\n","    random_state = 42\n",")\n","\n","# Target balancing\n","smote = SMOTE(random_state = 42)\n","X_train_res, y_train_res = smote.fit_resample(X_train, y_train)"],"metadata":{"id":"3vVPvJjS6BJX"},"id":"3vVPvJjS6BJX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","# This cell is a handshake more than anything else, just because I was rusty from not coding in a long time\n","\n","# RandomForest Training\n","rf_model = RandomForestClassifier(\n","    random_state = 42,\n","    n_estimators = 1\n",")\n","rf_model.fit(X_train_res, y_train_res)\n","\n","# Prediction\n","y_pred_rf = rf_model.predict(X_test)\n","\n","# Evaluating the model's performance\n","print(\"\\nRandomForest - Classification Report:\")\n","print(classification_report(y_test, y_pred_rf))\n","print(\"\\nRandomForest - Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred_rf))\n","\n","# Feature Importance (suggestion by Mistral AI's LeChat chatbot)\n","print(\"\\nRandomForest - Feature Importance:\")\n","for feature, importance in zip(X.columns, rf_model.feature_importances_):\n","    print(f\"{feature}: {importance:.4f}\")"],"metadata":{"id":"Mg-FiiQE8Zcj"},"id":"Mg-FiiQE8Zcj","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","\n","# Handshake-y, written with the help of Mistral AI's LeChat chatbot\n","\n","models = {\n","    \"Random Forest 1\": RandomForestClassifier(random_state = 42, n_estimators=1),\n","    \"Random Forest 50\": RandomForestClassifier(random_state = 42, n_estimators=50),\n","    \"Random Forest 100\": RandomForestClassifier(random_state = 42, n_estimators=100),\n","    \"Random Forest 150\": RandomForestClassifier(random_state = 42, n_estimators=150),\n","    \"Random Forest 200\": RandomForestClassifier(random_state = 42, n_estimators=200),\n","    \"Random Forest 500\": RandomForestClassifier(random_state = 42, n_estimators=500)#,\n","    #\"XGBoost\": XGBClassifier(random_state=42),\n","    #\"LightGBM\": LGBMClassifier(random_state=42)\n","}\n","\n","\n","results = {}\n","\n","\n","for name, model in models.items():\n","    print(f\"\\nTreinando {name}...\")\n","\n","\n","    start_time = time.time()\n","    model.fit(X_train_res, y_train_res)\n","    training_time = time.time() - start_time\n","\n","    # Predictions and Metrics\n","    y_pred = model.predict(X_test)\n","    report = classification_report(y_test, y_pred, output_dict=True)\n","    confusion = confusion_matrix(y_test, y_pred)\n","\n","    # Store Results\n","    results[name] = {\n","        \"training_time\": training_time,\n","        \"classification_report\": report,\n","        \"confusion_matrix\": confusion,\n","        \"model\": model\n","    }\n","\n","\n","    print(f\"Tempo de treinamento: {training_time:.2f} segundos\")\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, y_pred))\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, y_pred))\n","\n","# Print ALL models trained\n","results_df = pd.DataFrame({\n","    \"Modelo\": results.keys(),\n","    \"Tempo de Treinamento (s)\": [res[\"training_time\"] for res in results.values()],\n","    \"Acurácia\": [res[\"classification_report\"][\"accuracy\"] for res in results.values()],\n","    \"Precision (Classe 1)\": [res[\"classification_report\"][\"1\"][\"precision\"] for res in results.values()],\n","    \"Recall (Classe 1)\": [res[\"classification_report\"][\"1\"][\"recall\"] for res in results.values()],\n","    \"F1-Score (Classe 1)\": [res[\"classification_report\"][\"1\"][\"f1-score\"] for res in results.values()]\n","})\n","\n","print(\"\\nResumo dos Resultados:\")\n","results_df\n"],"metadata":{"id":"X9MmrTkOM1__"},"id":"X9MmrTkOM1__","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import RandomizedSearchCV\n","from scipy.stats import randint\n","\n","# Written with the help of Mistral AI's LeChat chatbot\n","param_dist = {\n","    'n_estimators': randint(30, 500),\n","    'max_depth': [None, 10, 20, 30],\n","    'min_samples_split': randint(2, 10),\n","    'max_features': ['sqrt', 'log2'],\n","    'bootstrap': [True]\n","}\n","\n","rf = RandomForestClassifier(random_state=42, oob_score=True)\n","rf_search = RandomizedSearchCV(rf, param_dist, n_iter = 200, cv=2, random_state=42, n_jobs=-1)\n","rf_search.fit(X_train_res, y_train_res)\n","\n","best_rf = rf_search.best_estimator_\n","print(f\"Melhores parâmetros: {rf_search.best_params_}\")\n","\n","\n","\n","\n","import pickle\n","with open('rf_search_models.pkl', 'wb') as file:\n","    pickle.dump(rf_search, file)\n","with open('best_random_forest_model.pkl', 'wb') as file:\n","    pickle.dump(best_rf, file)\n","\n","y_pred = best_rf.predict(X_test)\n","\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred))"],"metadata":{"id":"j0PuUjCTOVLm"},"id":"j0PuUjCTOVLm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This cell was written BY Mistral AI's LeChat chatbot, and it ran on the first try, look at that\n","\n","from scipy.stats import randint, uniform\n","from xgboost import XGBClassifier\n","\n","# Definir o espaço de busca para hiperparâmetros do XGBoost\n","param_dist = {\n","    'n_estimators': randint(30, 500),          # Número de árvores\n","    'max_depth': randint(3, 15),              # Profundidade máxima das árvores\n","    'learning_rate': uniform(0.01, 0.3),       # Taxa de aprendizado\n","    'subsample': uniform(0.6, 0.4),           # Fração de amostras usadas para treinar cada árvore\n","    'colsample_bytree': uniform(0.6, 0.4),    # Fração de features usadas para treinar cada árvore\n","    'gamma': uniform(0, 0.5),                  # Limiar mínimo de redução de perda para dividir um nó\n","    'min_child_weight': randint(1, 10),       # Soma mínima dos pesos das amostras em um nó filho\n","}\n","\n","# Criar o modelo base do XGBoost\n","xgb = XGBClassifier(\n","    random_state=42,\n","    eval_metric='logloss',  # Métrica de avaliação\n","    use_label_encoder=False  # Evita warnings sobre label encoding\n",")\n","\n","# RandomizedSearchCV para otimização do XGBoost\n","xgb_search = RandomizedSearchCV(\n","    xgb,\n","    param_distributions=param_dist,\n","    n_iter=200,  # Número de combinações a testar\n","    cv=2,        # Número de folds para validação cruzada\n","    random_state=42,\n","    n_jobs=-1,   # Usa todos os núcleos da CPU\n","    verbose=1   # Mostra o progresso\n",")\n","\n","# Treinar com os dados balanceados (SMOTE)\n","xgb_search.fit(X_train_res, y_train_res)\n","\n","# Melhor modelo\n","best_xgb = xgb_search.best_estimator_\n","print(f\"Melhores parâmetros: {xgb_search.best_params_}\")\n","\n","# Salvar o objeto completo do RandomizedSearchCV\n","with open('xgb_search_models.pkl', 'wb') as file:\n","    pickle.dump(xgb_search, file)\n","\n","# Salvar o melhor modelo\n","with open('best_xgboost_model.pkl', 'wb') as file:\n","    pickle.dump(best_xgb, file)\n","\n","# Previsões\n","y_pred = best_xgb.predict(X_test)\n","# report = classification_report(y_test, y_pred, output_dict=True)\n","# confusion = confusion_matrix(y_test, y_pred)\n","\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred))"],"metadata":{"id":"cpfYS7xmcbSW"},"id":"cpfYS7xmcbSW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This cell was written BY Mistral AI's LeChat chatbot, and it ran on the first try, look at that\n","from sklearn.svm import SVC\n","\n","# Definir o espaço de busca para hiperparâmetros do SVM\n","param_dist = {\n","    'C': uniform(0.1, 10),          # Parâmetro de regularização\n","    'gamma': ['scale', 'auto'] + list(uniform(0.001, 1).rvs(5)),  # Coeficiente do kernel RBF\n","    'kernel': ['linear', 'rbf', 'poly'],  # Tipo de kernel\n","    'degree': randint(2, 5),       # Grau do kernel polinomial (só aplicável se kernel='poly')\n","    'class_weight': ['balanced', None]  # Balanceamento de classes\n","}\n","\n","# Criar o modelo base do SVM\n","svm = SVC(random_state=42, probability=True)  # probability=True para obter probabilidades\n","\n","# RandomizedSearchCV para otimização do SVM\n","svm_search = RandomizedSearchCV(\n","    svm,\n","    param_distributions=param_dist,\n","    n_iter=50,  # Número de combinações a testar\n","    cv=2,        # Número de folds para validação cruzada\n","    random_state=42,\n","    n_jobs=-1,   # Usa todos os núcleos da CPU\n","    verbose=1   # Mostra o progresso\n",")\n","\n","# Treinar com os dados balanceados (SMOTE)\n","svm_search.fit(X_train_res, y_train_res)\n","\n","# Melhor modelo\n","best_svm = svm_search.best_estimator_\n","print(f\"Melhores parâmetros: {svm_search.best_params_}\")\n","\n","# Salvar o objeto completo do RandomizedSearchCV\n","with open('svm_search_models.pkl', 'wb') as file:\n","    pickle.dump(svm_search, file)\n","\n","# Salvar o melhor modelo\n","with open('best_svm_model.pkl', 'wb') as file:\n","    pickle.dump(best_svm, file)\n","\n","# Previsões\n","y_pred = best_svm.predict(X_test)\n","# report = classification_report(y_test, y_pred, output_dict=True)\n","# confusion = confusion_matrix(y_test, y_pred)\n","\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred))"],"metadata":{"id":"OhNqAy9Hh3gI"},"id":"OhNqAy9Hh3gI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving the processed data, just in case\n","with open('tess_clean_data.pkl', 'wb') as file:\n","    pickle.dump(data, file)\n","data.to_csv('tess_clean_data_no_index.csv', index=False)\n","data.to_csv('tess_clean_data_yes_index.csv', index=True)"],"metadata":{"id":"utNm0lL6mJmF"},"id":"utNm0lL6mJmF","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Comparing feature importance for each model.\n","print(\"RF:\")\n","for feature, importance in zip(X.columns, best_rf.feature_importances_):\n","    print(f\"{feature}: {importance:.4f}\")\n","\n","print(\"XBG:\")\n","for feature, importance in zip(X.columns, best_xgb.feature_importances_):\n","    print(f\"{feature}: {importance:.4f}\")\n","\n","print(\"SVC:\")\n","for feature, importance in zip(X.columns, best_svm.feature_importances_):\n","    print(f\"{feature}: {importance:.4f}\")"],"metadata":{"id":"ADm5vL1QnrFt"},"id":"ADm5vL1QnrFt","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Actually re-training the models for the web application, since inputing all 16 would be tedious\n","# This cell, as well as the next one, is just ctrl+c ctrl+v of previous ones\n","\n","reduced_features = ['st_dist', 'pl_rade', 'pl_eqt', 'pl_trandep', 'pl_insol']\n","\n","# Separa features de targets\n","X = data[reduced_features]\n","y = data['tfopwg_disp']\n","\n","# Normaliza as features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Train test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_scaled, y,\n","    test_size = 0.2,\n","    stratify = y,\n","    random_state = 42\n",")\n","\n","# Balanceamento dos targets no conjunto de treino\n","smote = SMOTE(random_state = 42)\n","X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n","\n","\n","from sklearn.model_selection import RandomizedSearchCV\n","from scipy.stats import randint\n","\n","param_dist = {\n","    'n_estimators': randint(30, 500),\n","    'max_depth': [None, 10, 20, 30],\n","    'min_samples_split': randint(2, 10),\n","    'max_features': ['sqrt', 'log2'],\n","    'bootstrap': [True]\n","}\n","\n","rf = RandomForestClassifier(random_state=42, oob_score=True)\n","rf_search = RandomizedSearchCV(rf, param_dist, n_iter = 200, cv=2, random_state=42, n_jobs=-1)\n","rf_search.fit(X_train_res, y_train_res)\n","\n","best_rf = rf_search.best_estimator_\n","print(f\"Melhores parâmetros: {rf_search.best_params_}\")\n","\n","import pickle\n","with open('rf_search_models_reduced_features.pkl', 'wb') as file:\n","    pickle.dump(rf_search, file)\n","with open('best_random_forest_model_reduced_features.pkl', 'wb') as file:\n","    pickle.dump(best_rf, file)\n","\n","\n","\n","\n","\n","from scipy.stats import randint, uniform\n","from xgboost import XGBClassifier\n","\n","# Definir o espaço de busca para hiperparâmetros do XGBoost\n","param_dist = {\n","    'n_estimators': randint(30, 500),          # Número de árvores\n","    'max_depth': randint(3, 15),              # Profundidade máxima das árvores\n","    'learning_rate': uniform(0.01, 0.3),       # Taxa de aprendizado\n","    'subsample': uniform(0.6, 0.4),           # Fração de amostras usadas para treinar cada árvore\n","    'colsample_bytree': uniform(0.6, 0.4),    # Fração de features usadas para treinar cada árvore\n","    'gamma': uniform(0, 0.5),                  # Limiar mínimo de redução de perda para dividir um nó\n","    'min_child_weight': randint(1, 10),       # Soma mínima dos pesos das amostras em um nó filho\n","}\n","\n","# Criar o modelo base do XGBoost\n","xgb = XGBClassifier(\n","    random_state=42,\n","    eval_metric='logloss',  # Métrica de avaliação\n","    use_label_encoder=False  # Evita warnings sobre label encoding\n",")\n","\n","# RandomizedSearchCV para otimização do XGBoost\n","xgb_search = RandomizedSearchCV(\n","    xgb,\n","    param_distributions=param_dist,\n","    n_iter=200,  # Número de combinações a testar\n","    cv=2,        # Número de folds para validação cruzada\n","    random_state=42,\n","    n_jobs=-1,   # Usa todos os núcleos da CPU\n","    verbose=1   # Mostra o progresso\n",")\n","\n","# Treinar com os dados balanceados (SMOTE)\n","xgb_search.fit(X_train_res, y_train_res)\n","\n","# Melhor modelo\n","best_xgb = xgb_search.best_estimator_\n","print(f\"Melhores parâmetros: {xgb_search.best_params_}\")\n","\n","# Salvar o objeto completo do RandomizedSearchCV\n","with open('xgb_search_models_reduced_features.pkl', 'wb') as file:\n","    pickle.dump(xgb_search, file)\n","\n","# Salvar o melhor modelo\n","with open('best_xgboost_model_reduced_features.pkl', 'wb') as file:\n","    pickle.dump(best_xgb, file)\n","\n","\n","\n","\n","from sklearn.svm import SVC\n","\n","# Definir o espaço de busca para hiperparâmetros do SVM\n","param_dist = {\n","    'C': uniform(0.1, 10),          # Parâmetro de regularização\n","    'gamma': ['scale', 'auto'] + list(uniform(0.001, 1).rvs(5)),  # Coeficiente do kernel RBF\n","    'kernel': ['linear', 'rbf', 'poly'],  # Tipo de kernel\n","    'degree': randint(2, 5),       # Grau do kernel polinomial (só aplicável se kernel='poly')\n","    'class_weight': ['balanced', None]  # Balanceamento de classes\n","}\n","\n","# Criar o modelo base do SVM\n","svm = SVC(random_state=42, probability=True)  # probability=True para obter probabilidades\n","\n","# RandomizedSearchCV para otimização do SVM\n","svm_search = RandomizedSearchCV(\n","    svm,\n","    param_distributions=param_dist,\n","    n_iter=200,  # Número de combinações a testar\n","    cv=2,        # Número de folds para validação cruzada\n","    random_state=42,\n","    n_jobs=-1,   # Usa todos os núcleos da CPU\n","    verbose=1   # Mostra o progresso\n",")\n","\n","# Treinar com os dados balanceados (SMOTE)\n","svm_search.fit(X_train_res, y_train_res)\n","\n","# Melhor modelo\n","best_svm = svm_search.best_estimator_\n","print(f\"Melhores parâmetros: {svm_search.best_params_}\")\n","\n","# Salvar o objeto completo do RandomizedSearchCV\n","with open('svm_search_models_reduced_features.pkl', 'wb') as file:\n","    pickle.dump(svm_search, file)\n","\n","# Salvar o melhor modelo\n","with open('best_svm_model_reduced_features.pkl', 'wb') as file:\n","    pickle.dump(best_svm, file)"],"metadata":{"id":"EJmqDLnCpQKV"},"id":"EJmqDLnCpQKV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Previsões\n","y_pred = best_rf.predict(X_test)\n","# report = classification_report(y_test, y_pred, output_dict=True)\n","# confusion = confusion_matrix(y_test, y_pred)\n","print(\"RANDOM FORESTS\")\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred))\n","\n","\n","\n","# Previsões\n","y_pred = best_xgb.predict(X_test)\n","# report = classification_report(y_test, y_pred, output_dict=True)\n","# confusion = confusion_matrix(y_test, y_pred)\n","print(\"\\nXGB\")\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred))\n","\n","\n","\n","\n","# Previsões\n","y_pred = best_svm.predict(X_test)\n","# report = classification_report(y_test, y_pred, output_dict=True)\n","# confusion = confusion_matrix(y_test, y_pred)\n","print(\"\\nSVM\")\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred))"],"metadata":{"id":"0WW1rxWxrdiN"},"id":"0WW1rxWxrdiN","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}